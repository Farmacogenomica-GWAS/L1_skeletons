**# L1_skeletons File Processing**

This document describes the processing of the `L1_skeletons.hdf5` file using Python and the `h5py`, `pandas`, and `numpy` libraries. The primary goal was to extract relevant data from the file and convert it into a more accessible CSV format. The `L1_skeletons.hdf5` file was obtained by using the Tierpsy Tracker.

**Key Steps:**

1.  **File Inspection:** The `L1_skeletons.hdf5` file was inspected to identify its structure and contents. This involved examining the groups and datasets within the file.
2.  **Dataset Extraction:**
    * The following datasets were extracted and converted to individual CSV files:
        * `blob_features`
        * `contour_area`
        * `contour_side1`
        * `contour_side1_length`
        * `contour_side2`
        * `contour_side2_length`
        * `contour_width`
        * `plate_worms`
        * `skeleton`
        * `skeleton_length`
        * `trajectories_data`
        * `width_midbody`
    * For the `contour_side1`, `contour_side2`, and `skeleton` datasets, which were originally 3D arrays, the data was reshaped into a 2D format to ensure compatibility with the CSV format.

3.  **Output Format:** All extracted data was saved as CSV files, with column headers derived from the dataset structure.  For the `contour_side1`, `contour_side2`, and `skeleton` datasets, the reshaped data was saved with new column names to reflect the new 2D structure.

**Libraries Used:**

* `h5py`: For reading and navigating the HDF5 file structure.
* `pandas`: For creating and exporting DataFrames to CSV files.
* `numpy`: For reshaping arrays during the extraction of the `contour_side1`, `contour_side2`, and `skeleton` datasets.

**# L1_skeletons File Processing**

This document describes the processing of the `L1_skeletons.hdf5` file, generated by the Tierpsy Tracker, using Python and the `h5py`, `pandas`, and `numpy` libraries. The primary goal was to selectively extract datasets residing within groups of the file and convert them into a more accessible CSV format.

**Key Steps:**

1.  **File Inspection:** The `L1_skeletons.hdf5` file was inspected to understand its hierarchical structure, including groups and the datasets contained within them.

2.  **Group Dataset Extraction:**
    * The following datasets, located within various groups, were extracted and converted to individual CSV files:
        * `intensity_analysis/switched_head_tail` (saved as `intensity_analysis_switched_head_tail.csv`)
        * `timestamp/raw` (saved as `timestamp_raw.csv`)
        * `timestamp/time` (saved as `timestamp_time.csv`)
    * For datasets within groups that were originally 3D arrays (e.g., if any existed within the explored groups), the data would have been reshaped into a 2D format for CSV compatibility.

3.  **Scalar Dataset Extraction from `provenance_tracking`:**
    * Scalar datasets (datasets containing a single value) specifically located within the `/provenance_tracking` group were extracted.
    * The names and values of these scalar datasets (`BLOB_FEATS`, `INT_SKE_ORIENT`, `SKE_CREATE`, `SKE_FILT`, `SKE_INIT`, `SKE_ORIENT`, `TRAJ_CREATE`, `TRAJ_JOIN`, and their counterparts prefixed with `provenance_tracking/`) were collected.
    * This scalar data was then saved into a single CSV file named `provenance_tracking_scalars.csv` with two columns: 'name' and 'value'.

4.  **Exclusion of Top-Level Datasets:** Datasets located directly at the root level of the HDF5 file (e.g., `blob_features`, `contour_area`, `contour_side1`, etc.) were explicitly skipped during this processing run.

5.  **Output Format:**
    * Datasets extracted from groups were saved as individual CSV files. The filename was constructed by replacing the group path's forward slashes (`/`) with underscores (`_`) and appending the dataset name (e.g., `intensity_analysis_switched_head_tail.csv`).
    * The reshaped data (if any within groups) was saved with column headers reflecting the new 2D structure.
    * Scalar datasets from `provenance_tracking` were saved in the `provenance_tracking_scalars.csv` file as described in step 3.

**Libraries Used:**

* `h5py`: For reading and navigating the HDF5 file structure, including accessing groups and datasets within them.
* `pandas`: For creating and exporting DataFrames to CSV files, both for the regular datasets and the scalar data.
* `numpy`: For reshaping arrays (if necessary for 3D datasets within groups) to ensure compatibility with the CSV format.

**Purpose:**

The processing steps outlined here were performed to make the data contained within the `L1_skeletons.hdf5` file more readily available for analysis. The conversion to CSV format allows for easier manipulation and exploration of the data using standard data analysis tools. The initial processing focused on extracting top-level datasets, while the subsequent processing targeted datasets specifically within groups and scalar data from the `provenance_tracking` group.
