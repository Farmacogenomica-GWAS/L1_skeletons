{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L1_intensities Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "mJ0cvs4-1Tx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "r-sUdzYd1WB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBwaBQyl1ZZO",
        "outputId": "4d75075a-537d-43d5-ecd2-70e051fa1c89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "6dejoB_01hGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "q2Dx2M-U1jWr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "00-qvt201lZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T9x75YW1nZ7",
        "outputId": "6b086325-72f9-45f7-ec06-818df70b2ac4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 21223 elements\n",
            "  Dataset: contour_area, Size: 21223 elements\n",
            "  Dataset: contour_side1, Size: 2079854 elements\n",
            "  Dataset: contour_side1_length, Size: 21223 elements\n",
            "  Dataset: contour_side2, Size: 2079854 elements\n",
            "  Dataset: contour_side2_length, Size: 21223 elements\n",
            "  Dataset: contour_width, Size: 1039927 elements\n",
            "  Dataset: plate_worms, Size: 24341 elements\n",
            "  Dataset: skeleton, Size: 2079854 elements\n",
            "  Dataset: skeleton_length, Size: 21223 elements\n",
            "  Dataset: trajectories_data, Size: 21223 elements\n",
            "  Dataset: width_midbody, Size: 21223 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "DUmTwgR31xLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # IMPORTANT NOTE:\n",
        "# Some reshaping was needed to convert the datasets into a 2D array for its correct extraction\n",
        "# Explanation:\n",
        "# Pandas DataFrames, which are used to create CSV files, require 2-dimensional data.\n",
        "# HDF5 datasets, however, can have any number of dimensions (1D, 2D, 3D, etc.).\n",
        "# When a dataset has more than 2 dimensions, we need to reshape it into a 2D\n",
        "# structure so that it can be stored in a DataFrame and then written to a CSV file."
      ],
      "metadata": {
        "id": "T5Ba9eQMsQae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file (including those within groups)\n",
        "    to individual CSV files. Handles datasets with more than 2 dimensions\n",
        "    by reshaping them. Exports scalar datasets from 'provenance_tracking'\n",
        "    to a separate CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    scalar_data = []\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        def process_hdf5_item(name, obj):\n",
        "            nonlocal scalar_data\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(f\"\\nProcessing dataset: {name}\")\n",
        "                if len(obj.shape) > 0:  # Check if the dataset has dimensions\n",
        "                    data = obj[:]  # Read all data\n",
        "\n",
        "                    if len(data.shape) > 2:\n",
        "                        # Reshape the data to 2D\n",
        "                        original_shape = data.shape\n",
        "                        new_shape = (original_shape[0], np.prod(original_shape[1:]))\n",
        "                        data = data.reshape(new_shape)\n",
        "                        print(f\"  Reshaped dataset '{name}' from {original_shape} to {new_shape}\")\n",
        "\n",
        "                    # Convert the data to a Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the output CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{name.replace('/', '_')}.csv\")\n",
        "                    # Replace '/' with '_' in the filename to avoid directory issues\n",
        "\n",
        "                    # Save the DataFrame to a CSV file\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"  Dataset '{name}' successfully exported to: {csv_file_path}\")\n",
        "                elif obj.parent.name == '/provenance_tracking':\n",
        "                    scalar_data.append({'name': name, 'value': obj[()]})\n",
        "                    print(f\"  Found scalar dataset in 'provenance_tracking': {name} = {obj[()]}\")\n",
        "                else:\n",
        "                    print(f\"  Skipping scalar dataset: {name} with shape {obj.shape}\")\n",
        "            elif isinstance(obj, h5py.Group):\n",
        "                print(f\"\\nExploring group: {name}\")\n",
        "                # Recursively process items within the group\n",
        "                obj.visititems(process_hdf5_item)\n",
        "            else:\n",
        "                print(f\"  Skipping non-dataset/group: {name}\")\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            hdf_file.visititems(process_hdf5_item)\n",
        "\n",
        "        # Save scalar data from 'provenance_tracking' to a CSV\n",
        "        if scalar_data:\n",
        "            scalar_df = pd.DataFrame(scalar_data)\n",
        "            scalar_csv_path = os.path.join(output_dir, \"provenance_tracking_scalars.csv\")\n",
        "            scalar_df.to_csv(scalar_csv_path, index=False)\n",
        "            print(f\"\\nScalar datasets from 'provenance_tracking' exported to: {scalar_csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKsDkf6ruLA7",
        "outputId": "b1eb428c-08e7-440c-d1c5-7beeef671c39"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "  Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/blob_features.csv\n",
            "\n",
            "Processing dataset: contour_area\n",
            "  Dataset 'contour_area' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_area.csv\n",
            "\n",
            "Processing dataset: contour_side1\n",
            "  Reshaped dataset 'contour_side1' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'contour_side1' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side1.csv\n",
            "\n",
            "Processing dataset: contour_side1_length\n",
            "  Dataset 'contour_side1_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side1_length.csv\n",
            "\n",
            "Processing dataset: contour_side2\n",
            "  Reshaped dataset 'contour_side2' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'contour_side2' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side2.csv\n",
            "\n",
            "Processing dataset: contour_side2_length\n",
            "  Dataset 'contour_side2_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side2_length.csv\n",
            "\n",
            "Processing dataset: contour_width\n",
            "  Dataset 'contour_width' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_width.csv\n",
            "\n",
            "Exploring group: intensity_analysis\n",
            "\n",
            "Processing dataset: switched_head_tail\n",
            "  Dataset 'switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/switched_head_tail.csv\n",
            "\n",
            "Processing dataset: intensity_analysis/switched_head_tail\n",
            "  Dataset 'intensity_analysis/switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/intensity_analysis_switched_head_tail.csv\n",
            "\n",
            "Processing dataset: plate_worms\n",
            "  Dataset 'plate_worms' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/plate_worms.csv\n",
            "\n",
            "Exploring group: provenance_tracking\n",
            "\n",
            "Processing dataset: BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: skeleton\n",
            "  Reshaped dataset 'skeleton' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'skeleton' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/skeleton.csv\n",
            "\n",
            "Processing dataset: skeleton_length\n",
            "  Dataset 'skeleton_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/skeleton_length.csv\n",
            "\n",
            "Exploring group: timestamp\n",
            "\n",
            "Processing dataset: raw\n",
            "  Dataset 'raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/raw.csv\n",
            "\n",
            "Processing dataset: time\n",
            "  Dataset 'time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/time.csv\n",
            "\n",
            "Processing dataset: timestamp/raw\n",
            "  Dataset 'timestamp/raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_raw.csv\n",
            "\n",
            "Processing dataset: timestamp/time\n",
            "  Dataset 'timestamp/time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_time.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "  Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/trajectories_data.csv\n",
            "\n",
            "Processing dataset: width_midbody\n",
            "  Dataset 'width_midbody' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/width_midbody.csv\n",
            "\n",
            "Scalar datasets from 'provenance_tracking' exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/provenance_tracking_scalars.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    }
  ]
}