{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis of the Results File: L1_intensities Obtained via Tierpsy Tracker"
      ],
      "metadata": {
        "id": "mJ0cvs4-1Tx-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Drive connection"
      ],
      "metadata": {
        "id": "r-sUdzYd1WB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBwaBQyl1ZZO",
        "outputId": "2070f75a-638a-440b-99c8-21313c33a0b5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Libraries"
      ],
      "metadata": {
        "id": "6dejoB_01hGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "q2Dx2M-U1jWr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting file content"
      ],
      "metadata": {
        "id": "00-qvt201lZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inspect_hdf5_datasets(file_path):\n",
        "    \"\"\"\n",
        "    Inspects an HDF5 file and prints the names and sizes of its datasets.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Inspecting file: {file_path}\")\n",
        "    print(f\"{'='*30}\")\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return  # Exit the function if the file doesn't exist\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdfid:\n",
        "            datasets_info = {}\n",
        "            for name, obj in hdfid.items():\n",
        "                if isinstance(obj, h5py.Dataset):\n",
        "                    datasets_info[name] = obj.size\n",
        "\n",
        "            if datasets_info:\n",
        "                print(\"Datasets found and their sizes:\")\n",
        "                for name, size in datasets_info.items():\n",
        "                    print(f\"  Dataset: {name}, Size: {size} elements\")\n",
        "            else:\n",
        "                print(\"No top-level datasets were found in this file.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing the file '{file_path}': {e}\")\n",
        "\n",
        "# Define the file path you want to analyze here.  REPLACE THIS!\n",
        "file_to_analyze = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5'  # <--- REPLACE THIS LINE\n",
        "\n",
        "inspect_hdf5_datasets(file_to_analyze)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T9x75YW1nZ7",
        "outputId": "dfed25e0-ee3a-4cce-a219-70cd1b3a06c4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Inspecting file: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5\n",
            "==============================\n",
            "Datasets found and their sizes:\n",
            "  Dataset: blob_features, Size: 21223 elements\n",
            "  Dataset: contour_area, Size: 21223 elements\n",
            "  Dataset: contour_side1, Size: 2079854 elements\n",
            "  Dataset: contour_side1_length, Size: 21223 elements\n",
            "  Dataset: contour_side2, Size: 2079854 elements\n",
            "  Dataset: contour_side2_length, Size: 21223 elements\n",
            "  Dataset: contour_width, Size: 1039927 elements\n",
            "  Dataset: plate_worms, Size: 24341 elements\n",
            "  Dataset: skeleton, Size: 2079854 elements\n",
            "  Dataset: skeleton_length, Size: 21223 elements\n",
            "  Dataset: trajectories_data, Size: 21223 elements\n",
            "  Dataset: width_midbody, Size: 21223 elements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exporting the datasets to individuals CSV files"
      ],
      "metadata": {
        "id": "DUmTwgR31xLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # IMPORTANT NOTE:\n",
        "# Some reshaping was needed to convert the datasets into a 2D array for its correct extraction\n",
        "# Explanation:\n",
        "# Pandas DataFrames, which are used to create CSV files, require 2-dimensional data.\n",
        "# HDF5 datasets, however, can have any number of dimensions (1D, 2D, 3D, etc.).\n",
        "# When a dataset has more than 2 dimensions, we need to reshape it into a 2D\n",
        "# structure so that it can be stored in a DataFrame and then written to a CSV file."
      ],
      "metadata": {
        "id": "T5Ba9eQMsQae"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def export_hdf5_datasets_to_csv(file_path, output_dir):\n",
        "    \"\"\"\n",
        "    Exports all datasets from an HDF5 file (including those within groups)\n",
        "    to individual CSV files. Handles datasets with more than 2 dimensions\n",
        "    by reshaping them. Exports scalar datasets from 'provenance_tracking'\n",
        "    to a separate CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The path to the HDF5 file.\n",
        "        output_dir (str): The path to the directory where the CSV files will be saved.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*30}\")\n",
        "    print(f\"Exporting all datasets from: {file_path}\")\n",
        "    print(f\"CSV files will be saved in: {output_dir}\")\n",
        "    print(f\"{'='*30}\")\n",
        "\n",
        "    scalar_data = []\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Error: The file '{file_path}' was not found.\")\n",
        "            return\n",
        "\n",
        "        # Create the output directory if it doesn't exist\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        def process_hdf5_item(name, obj):\n",
        "            nonlocal scalar_data\n",
        "            if isinstance(obj, h5py.Dataset):\n",
        "                print(f\"\\nProcessing dataset: {name}\")\n",
        "                if len(obj.shape) > 0:  # Check if the dataset has dimensions\n",
        "                    data = obj[:]  # Read all data\n",
        "\n",
        "                    if len(data.shape) > 2:\n",
        "                        # Reshape the data to 2D\n",
        "                        original_shape = data.shape\n",
        "                        new_shape = (original_shape[0], np.prod(original_shape[1:]))\n",
        "                        data = data.reshape(new_shape)\n",
        "                        print(f\"  Reshaped dataset '{name}' from {original_shape} to {new_shape}\")\n",
        "\n",
        "                    # Convert the data to a Pandas DataFrame\n",
        "                    df = pd.DataFrame(data)\n",
        "\n",
        "                    # Construct the output CSV file path\n",
        "                    csv_file_path = os.path.join(output_dir, f\"{name.replace('/', '_')}.csv\")\n",
        "                    # Replace '/' with '_' in the filename to avoid directory issues\n",
        "\n",
        "                    # Save the DataFrame to a CSV file\n",
        "                    df.to_csv(csv_file_path, index=False)\n",
        "                    print(f\"  Dataset '{name}' successfully exported to: {csv_file_path}\")\n",
        "                elif obj.parent.name == '/provenance_tracking':\n",
        "                    scalar_data.append({'name': name, 'value': obj[()]})\n",
        "                    print(f\"  Found scalar dataset in 'provenance_tracking': {name} = {obj[()]}\")\n",
        "                else:\n",
        "                    print(f\"  Skipping scalar dataset: {name} with shape {obj.shape}\")\n",
        "            elif isinstance(obj, h5py.Group):\n",
        "                print(f\"\\nExploring group: {name}\")\n",
        "                # Recursively process items within the group\n",
        "                obj.visititems(process_hdf5_item)\n",
        "            else:\n",
        "                print(f\"  Skipping non-dataset/group: {name}\")\n",
        "\n",
        "        with h5py.File(file_path, 'r') as hdf_file:\n",
        "            hdf_file.visititems(process_hdf5_item)\n",
        "\n",
        "        # Save scalar data from 'provenance_tracking' to a CSV\n",
        "        if scalar_data:\n",
        "            scalar_df = pd.DataFrame(scalar_data)\n",
        "            scalar_csv_path = os.path.join(output_dir, \"provenance_tracking_scalars.csv\")\n",
        "            scalar_df.to_csv(scalar_csv_path, index=False)\n",
        "            print(f\"\\nScalar datasets from 'provenance_tracking' exported to: {scalar_csv_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "    print(\"\\nData export process complete.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage:  MODIFY THESE PATHS APPROPRIATELY\n",
        "    hdf5_file_path = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5'  # <--- Replace with your HDF5 file path\n",
        "    csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets'  # <--- Replace with the desired output folder\n",
        "\n",
        "    export_hdf5_datasets_to_csv(hdf5_file_path, csv_output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKsDkf6ruLA7",
        "outputId": "84414009-4f26-44f3-f6b9-235aef480cae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Exporting all datasets from: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/L1_skeletons.hdf5\n",
            "CSV files will be saved in: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets\n",
            "==============================\n",
            "\n",
            "Processing dataset: blob_features\n",
            "  Dataset 'blob_features' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/blob_features.csv\n",
            "\n",
            "Processing dataset: contour_area\n",
            "  Dataset 'contour_area' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_area.csv\n",
            "\n",
            "Processing dataset: contour_side1\n",
            "  Reshaped dataset 'contour_side1' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'contour_side1' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side1.csv\n",
            "\n",
            "Processing dataset: contour_side1_length\n",
            "  Dataset 'contour_side1_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side1_length.csv\n",
            "\n",
            "Processing dataset: contour_side2\n",
            "  Reshaped dataset 'contour_side2' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'contour_side2' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side2.csv\n",
            "\n",
            "Processing dataset: contour_side2_length\n",
            "  Dataset 'contour_side2_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_side2_length.csv\n",
            "\n",
            "Processing dataset: contour_width\n",
            "  Dataset 'contour_width' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/contour_width.csv\n",
            "\n",
            "Exploring group: intensity_analysis\n",
            "\n",
            "Processing dataset: switched_head_tail\n",
            "  Dataset 'switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/switched_head_tail.csv\n",
            "\n",
            "Processing dataset: intensity_analysis/switched_head_tail\n",
            "  Dataset 'intensity_analysis/switched_head_tail' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/intensity_analysis_switched_head_tail.csv\n",
            "\n",
            "Processing dataset: plate_worms\n",
            "  Dataset 'plate_worms' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/plate_worms.csv\n",
            "\n",
            "Exploring group: provenance_tracking\n",
            "\n",
            "Processing dataset: BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/BLOB_FEATS\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/BLOB_FEATS = b'{\"func_name\": \"getBlobsFeats\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"strel_size\\\\\": 5}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/INT_SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/INT_SKE_ORIENT = b'{\"func_name\": \"correctHeadTailIntensity\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"intensities_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_intensities.hdf5\\\\\", \\\\\"smooth_W\\\\\": -1, \\\\\"gap_size\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"local_avg_win\\\\\": -1, \\\\\"min_frac_in\\\\\": 0.85, \\\\\"head_tail_param\\\\\": {\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1}, \\\\\"head_tail_int_method\\\\\": \\\\\"MEDIAN_INT\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_CREATE = b'{\"func_name\": \"trajectories2Skeletons\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"resampling_N\\\\\": 49, \\\\\"worm_midbody\\\\\": [0.33, 0.67], \\\\\"min_blob_area\\\\\": 25, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"skel_args\\\\\": {\\\\\"num_segments\\\\\": 24, \\\\\"head_angle_thresh\\\\\": 60}}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_FILT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_FILT = b'{\"func_name\": \"getFilteredSkels\", \"func_arguments\": \"{\\\\\"bad_seg_thresh\\\\\": 0.8, \\\\\"max_width_ratio\\\\\": 2.25, \\\\\"max_area_ratio\\\\\": 6, \\\\\"min_displacement\\\\\": 10, \\\\\"critical_alpha\\\\\": 0.01, \\\\\"min_num_skel\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_INIT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_INIT = b'{\"func_name\": \"processTrajectoryData\", \"func_arguments\": \"{\\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"smoothed_traj_param\\\\\": {\\\\\"min_track_size\\\\\": 0, \\\\\"displacement_smooth_win\\\\\": -1, \\\\\"threshold_smooth_win\\\\\": -1, \\\\\"roi_size\\\\\": -1}, \\\\\"filter_model_name\\\\\": \\\\\"/home/sirjlister/tierpsy-tracker/tierpsy/extras/model_state_isworm_20200615.pth\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/SKE_ORIENT\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/SKE_ORIENT = b'{\"func_name\": \"correctHeadTail\", \"func_arguments\": \"{\\\\\"max_gap_allowed\\\\\": -1, \\\\\"window_std\\\\\": -1, \\\\\"segment4angle\\\\\": -1, \\\\\"min_block_size\\\\\": -1, \\\\\"skeletons_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\"}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_CREATE\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_CREATE = b'{\"func_name\": \"getBlobsTable\", \"func_arguments\": \"{\\\\\"masked_image_file\\\\\": \\\\\"/home/sirjlister/Tmp/MaskedVideos/L1.hdf5\\\\\", \\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"min_area\\\\\": 25, \\\\\"min_box_width\\\\\": 5, \\\\\"worm_bw_thresh_factor\\\\\": 1.05, \\\\\"strel_size\\\\\": 5, \\\\\"analysis_type\\\\\": \\\\\"TIERPSY\\\\\", \\\\\"thresh_block_size\\\\\": 61, \\\\\"n_cores_used\\\\\": 1, \\\\\"buffer_size\\\\\": 10}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: provenance_tracking/TRAJ_JOIN\n",
            "  Found scalar dataset in 'provenance_tracking': provenance_tracking/TRAJ_JOIN = b'{\"func_name\": \"joinBlobsTrajectories\", \"func_arguments\": \"{\\\\\"trajectories_file\\\\\": \\\\\"/home/sirjlister/Tmp/Results/L1_skeletons.hdf5\\\\\", \\\\\"max_allowed_dist\\\\\": 25, \\\\\"min_track_size\\\\\": 0, \\\\\"max_frames_gap\\\\\": 0, \\\\\"area_ratio_lim\\\\\": 2, \\\\\"is_one_worm\\\\\": false, \\\\\"is_WT2\\\\\": false}\", \"pkgs_versions\": {\"tierpsy\": \"1.5.3a+ca1a4c5\", \"open_worm_analysis_toolbox\": \"\", \"tierpsy_features\": \"\"}, \"cmd_original\": \"/home/sirjlister/tierpsy-tracker/tierpsy/processing/ProcessWorker.py /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/L1.avi --masks_dir /home/sirjlister/Tmp/MaskedVideos/ --results_dir /home/sirjlister/Tmp/Results/ --json_file /home/sirjlister/Documentos/JL/GM-LIIGH/Proyectos/duplication/tierpsy/n2.json --analysis_checkpoints COMPRESS TRAJ_CREATE TRAJ_JOIN SKE_INIT BLOB_FEATS SKE_CREATE SKE_FILT SKE_ORIENT INT_PROFILE INT_SKE_ORIENT FEAT_INIT FEAT_TIERPSY\"}'\n",
            "\n",
            "Processing dataset: skeleton\n",
            "  Reshaped dataset 'skeleton' from (21223, 49, 2) to (21223, np.int64(98))\n",
            "  Dataset 'skeleton' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/skeleton.csv\n",
            "\n",
            "Processing dataset: skeleton_length\n",
            "  Dataset 'skeleton_length' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/skeleton_length.csv\n",
            "\n",
            "Exploring group: timestamp\n",
            "\n",
            "Processing dataset: raw\n",
            "  Dataset 'raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/raw.csv\n",
            "\n",
            "Processing dataset: time\n",
            "  Dataset 'time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/time.csv\n",
            "\n",
            "Processing dataset: timestamp/raw\n",
            "  Dataset 'timestamp/raw' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_raw.csv\n",
            "\n",
            "Processing dataset: timestamp/time\n",
            "  Dataset 'timestamp/time' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_time.csv\n",
            "\n",
            "Processing dataset: trajectories_data\n",
            "  Dataset 'trajectories_data' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/trajectories_data.csv\n",
            "\n",
            "Processing dataset: width_midbody\n",
            "  Dataset 'width_midbody' successfully exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/width_midbody.csv\n",
            "\n",
            "Scalar datasets from 'provenance_tracking' exported to: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/provenance_tracking_scalars.csv\n",
            "\n",
            "Data export process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization of each dataset"
      ],
      "metadata": {
        "id": "xqffKibmUBFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Top-Level Datasets"
      ],
      "metadata": {
        "id": "zWGs1kqTUF5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpmtF4u-T-DE",
        "outputId": "cac9a780-7c56-4600-fb27-0b84e061a226"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets\n",
            "==============================\n",
            "\n",
            "--- File: switched_head_tail.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  ini_frame  last_frame\n",
            "0         250        586         598\n",
            "1         256        458         492\n",
            "2         305        629         639\n",
            "3         360        570         578\n",
            "\n",
            "--- File: plate_worms.csv ---\n",
            "First 5 rows:\n",
            "   worm_index_blob  worm_index_joined  frame_number    coord_x    coord_y  \\\n",
            "0                1                  1             0  1054.8000  1563.1000   \n",
            "1                1                  1             1  1054.5403  1563.9948   \n",
            "2                1                  1             2  1054.6091  1564.3552   \n",
            "3                1                  1             3  1054.2062  1566.0361   \n",
            "4                1                  1             4  1053.9940  1566.3773   \n",
            "\n",
            "   box_length  box_width      angle   area  bounding_box_xmin  \\\n",
            "0   47.117940  16.760070 -71.565050  329.0             1041.0   \n",
            "1   46.595085  16.355530 -70.906510  315.0             1040.0   \n",
            "2   46.297005  17.018404 -69.676865  305.5             1039.0   \n",
            "3   47.822807  17.667025 -66.037510  322.0             1037.0   \n",
            "4   46.973965  17.720499 -66.250500  327.0             1037.0   \n",
            "\n",
            "   bounding_box_xmax  bounding_box_ymin  bounding_box_ymax  threshold  \n",
            "0             1071.0             1543.0             1586.0      126.0  \n",
            "1             1070.0             1544.0             1586.0      126.0  \n",
            "2             1070.0             1545.0             1585.0      126.0  \n",
            "3             1069.0             1546.0             1586.0      126.0  \n",
            "4             1068.0             1546.0             1586.0      126.0  \n",
            "\n",
            "--- File: skeleton.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1040.0  1581.0  1041.0000  1581.4764  1041.9529  1582.0000  1042.3036   \n",
            "1  1041.0  1577.0  1041.9957  1578.0043  1041.0085  1579.0085  1042.0000   \n",
            "2  1038.0  1583.0  1039.3439  1582.6561  1040.5583  1582.0000  1041.7386   \n",
            "3  1036.0  1584.0  1037.4410  1584.0000  1038.8821  1584.0000  1039.9355   \n",
            "4  1036.0  1582.0  1037.2968  1582.4323  1038.5935  1582.8645  1039.9385   \n",
            "\n",
            "           7          8          9  ...         88         89         90  \\\n",
            "0  1582.6964  1043.4916  1582.0000  ...  1065.5310  1546.4690  1066.5751   \n",
            "1  1580.0181  1042.4382  1581.0000  ...  1065.3972  1548.6028  1066.4015   \n",
            "2  1581.2614  1042.1167  1580.0000  ...  1065.5032  1548.4968  1066.5542   \n",
            "3  1583.0645  1040.9545  1582.0455  ...  1066.1603  1546.8397  1067.1792   \n",
            "4  1583.0000  1041.3053  1583.0000  ...  1063.7750  1549.0000  1064.8075   \n",
            "\n",
            "          91         92         93         94         95      96      97  \n",
            "0  1545.4249  1067.8755  1545.0000  1068.9559  1544.0441  1070.0  1543.0  \n",
            "1  1547.5985  1067.4058  1546.5942  1068.0000  1545.4202  1068.0  1544.0  \n",
            "2  1547.4458  1067.6052  1546.3948  1068.6561  1545.3439  1069.0  1544.0  \n",
            "3  1545.8208  1067.9114  1544.7341  1067.4557  1543.3671  1067.0  1542.0  \n",
            "4  1548.1925  1065.7740  1547.2260  1067.0334  1546.9666  1068.0  1546.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: skeleton_length.csv ---\n",
            "First 5 rows:\n",
            "           0\n",
            "0  70.870056\n",
            "1  68.170620\n",
            "2  71.342190\n",
            "3  69.168630\n",
            "4  65.611550\n",
            "\n",
            "--- File: raw.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: time.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: trajectories_data.csv ---\n",
            "First 5 rows:\n",
            "   frame_number  worm_index_joined  plate_worm_id  skeleton_id    coord_x  \\\n",
            "0             0                  1              0            0  1054.9213   \n",
            "1             1                  1              1            1  1054.5070   \n",
            "2             2                  1              2            2  1054.2874   \n",
            "3             3                  1              3            3  1054.2156   \n",
            "4             4                  1              4            4  1054.2451   \n",
            "\n",
            "     coord_y  threshold  has_skeleton  roi_size   area  timestamp_raw  \\\n",
            "0  1562.7554     127.05             1      83.0  322.0            NaN   \n",
            "1  1564.2189     127.05             1      83.0  322.0            NaN   \n",
            "2  1565.1562     127.05             1      83.0  317.0            NaN   \n",
            "3  1565.6821     127.05             1      83.0  317.0            NaN   \n",
            "4  1565.9114     127.05             1      83.0  317.0            NaN   \n",
            "\n",
            "   timestamp_time  is_good_skel  skel_outliers_flag  int_map_id  \n",
            "0             NaN             1                   0           0  \n",
            "1             NaN             1                   0           1  \n",
            "2             NaN             1                   0           2  \n",
            "3             NaN             1                   0           3  \n",
            "4             NaN             0                   0          -1  \n",
            "\n",
            "--- File: width_midbody.csv ---\n",
            "First 5 rows:\n",
            "          0\n",
            "0  7.940934\n",
            "1  6.587952\n",
            "2  7.008095\n",
            "3  6.681432\n",
            "4  9.063066\n",
            "\n",
            "--- File: blob_features.csv ---\n",
            "First 5 rows:\n",
            "     coord_x    coord_y   area  perimeter  box_length  box_width  quirkiness  \\\n",
            "0  1054.5870  1563.4897  410.5  137.43860   49.013310  17.635386    0.933026   \n",
            "1  1054.5862  1564.4614  373.5  137.78174   48.504364  17.176775    0.935197   \n",
            "2  1054.7413  1564.1034  396.0  143.68124   48.977306  18.514212    0.925799   \n",
            "3  1054.7122  1564.7903  396.5  144.26703   52.521960  18.717934    0.934340   \n",
            "4  1053.7576  1566.3472  425.0  130.71068   48.862236  19.241873    0.919197   \n",
            "\n",
            "   compactness  box_orientation  solidity  intensity_mean  intensity_std  \\\n",
            "0     0.273090       -70.144790  0.586429       115.01068       9.011264   \n",
            "1     0.247239       -69.145540  0.569360       114.46296       8.518961   \n",
            "2     0.241049       -66.801410  0.555400       115.23465       8.603841   \n",
            "3     0.239397       -65.224850  0.543151       114.84026       8.682251   \n",
            "4     0.312591       -65.376434  0.605845       115.91061       9.280919   \n",
            "\n",
            "        hu0       hu1       hu2       hu3       hu4       hu5           hu6  \n",
            "0  0.460622  0.147170  0.003798  0.001096  0.000002  0.000167 -1.119919e-06  \n",
            "1  0.494695  0.165938  0.004146  0.004529  0.000020  0.001832  4.671204e-07  \n",
            "2  0.469125  0.141980  0.005847  0.004134  0.000020  0.001557  4.720160e-07  \n",
            "3  0.523889  0.183997  0.012585  0.006046  0.000052  0.002540  6.977067e-06  \n",
            "4  0.399157  0.100433  0.006023  0.002341  0.000008  0.000664  2.250458e-06  \n",
            "\n",
            "--- File: contour_area.csv ---\n",
            "First 5 rows:\n",
            "           0\n",
            "0  409.34800\n",
            "1  372.68857\n",
            "2  395.40524\n",
            "3  395.65347\n",
            "4  423.88995\n",
            "\n",
            "--- File: contour_side1.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1040.0  1581.0  1040.3788  1580.0000  1041.7577  1580.0000  1043.1365   \n",
            "1  1041.0  1577.0  1041.1968  1576.1968  1042.1006  1577.1006  1043.0044   \n",
            "2  1038.0  1583.0  1037.0497  1581.9503  1038.1405  1581.0000  1039.4419   \n",
            "3  1036.0  1584.0  1036.2776  1582.7224  1037.2622  1581.7378  1038.3491   \n",
            "4  1036.0  1582.0  1036.8916  1581.1084  1038.1078  1581.0000  1039.3688   \n",
            "\n",
            "           7          8          9  ...         88         89         90  \\\n",
            "0  1580.0000  1044.3644  1579.6356  ...  1064.6356  1543.3644  1065.8635   \n",
            "1  1578.0044  1043.9083  1578.9083  ...  1063.7988  1546.2012  1064.9938   \n",
            "2  1580.5581  1040.4916  1579.5084  ...  1064.2155  1546.7845  1065.3750   \n",
            "3  1581.0000  1039.5244  1580.4756  ...  1063.0614  1545.9386  1064.0460   \n",
            "4  1581.0000  1040.6296  1581.0000  ...  1063.2618  1546.7382  1064.2170   \n",
            "\n",
            "         91         92         93         94         95      96      97  \n",
            "0  1543.000  1067.2423  1543.0000  1068.6212  1543.0000  1070.0  1543.0  \n",
            "1  1546.000  1066.1923  1545.8077  1067.0962  1544.9038  1068.0  1544.0  \n",
            "2  1546.000  1066.6078  1545.3922  1067.6575  1544.3425  1069.0  1544.0  \n",
            "3  1544.954  1065.0306  1543.9694  1066.0154  1542.9846  1067.0  1542.0  \n",
            "4  1546.000  1065.4780  1546.0000  1066.7390  1546.0000  1068.0  1546.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: contour_side1_length.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: contour_side2.csv ---\n",
            "First 5 rows:\n",
            "        0       1          2          3          4          5          6  \\\n",
            "0  1040.0  1581.0  1041.0000  1582.0703  1041.3922  1583.3922  1042.0000   \n",
            "1  1041.0  1577.0  1041.0000  1578.5922  1040.1625  1579.8375  1040.2562   \n",
            "2  1038.0  1583.0  1039.0947  1584.0000  1040.6035  1584.0000  1042.0795   \n",
            "3  1036.0  1584.0  1037.1406  1585.1406  1038.3977  1586.0000  1039.7147   \n",
            "4  1036.0  1582.0  1036.3268  1583.3268  1037.0000  1584.5101  1037.6875   \n",
            "\n",
            "           7          8          9  ...         88         89         90  \\\n",
            "0  1584.6250  1043.0774  1584.9226  ...  1068.5084  1547.4916  1069.0000   \n",
            "1  1581.2562  1041.3822  1582.3822  ...  1067.3250  1548.6750  1068.4508   \n",
            "2  1584.0795  1043.2072  1585.0000  ...  1067.1465  1547.8535  1068.2134   \n",
            "3  1585.2853  1041.2096  1585.0000  ...  1068.2660  1546.7340  1069.0000   \n",
            "4  1585.6875  1039.0143  1585.9857  ...  1064.5715  1550.4285  1065.6053   \n",
            "\n",
            "          91         92         93         94         95      96      97  \n",
            "0  1546.2108  1069.9006  1545.0994  1070.9503  1544.0497  1070.0  1543.0  \n",
            "1  1547.5492  1069.0000  1546.1844  1069.0000  1544.5922  1068.0  1544.0  \n",
            "2  1546.7866  1069.2803  1545.7197  1070.0000  1544.5089  1069.0  1544.0  \n",
            "3  1545.4249  1069.0000  1543.8119  1068.4335  1542.4335  1067.0  1542.0  \n",
            "4  1549.3947  1066.6393  1548.3607  1067.6732  1547.3268  1068.0  1546.0  \n",
            "\n",
            "[5 rows x 98 columns]\n",
            "\n",
            "--- File: contour_side2_length.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: contour_width.csv ---\n",
            "First 5 rows:\n",
            "              0         1         2         3         4         5         6  \\\n",
            "0  4.440892e-16  2.476459  4.070229  5.268222  4.300544  5.591248  7.206855   \n",
            "1  2.220446e-16  2.241886  4.477462  6.018023  7.465528  8.914824  9.958791   \n",
            "2  6.661338e-16  3.343863  4.558378  5.799832  8.391061  9.303346  9.474692   \n",
            "3  6.661338e-16  3.882026  5.000000  6.012957  8.486472  6.877194  6.196811   \n",
            "4  3.510834e-16  2.161270  4.322540  4.144590  4.037591  5.268573  6.429285   \n",
            "\n",
            "           7          8         9  ...        39        40        41  \\\n",
            "0   7.142036   6.716628  6.368028  ...  6.080514  5.457973  5.804826   \n",
            "1   9.959796   8.572754  9.219544  ...  5.302447  5.273197  5.019934   \n",
            "2  10.119326  10.722472  7.823526  ...  5.030927  4.786170  5.000000   \n",
            "3   6.021408   5.979192  6.707610  ...  5.099019  4.656916  4.242641   \n",
            "4   8.004076   8.485281  8.521736  ...  6.306495  5.619956  5.049779   \n",
            "\n",
            "         42        43        44        45        46        47   48  \n",
            "0  5.656854  5.656854  5.656854  5.656854  4.418699  2.890673  0.0  \n",
            "1  5.000000  4.439654  4.242641  3.986869  2.828427  2.008496  0.0  \n",
            "2  5.000000  3.505648  3.605551  2.846587  2.236068  2.236068  0.0  \n",
            "3  4.242641  4.242641  4.242641  4.242641  3.866644  1.933322  0.0  \n",
            "4  5.148320  5.000000  4.413038  3.728217  3.605551  2.161270  0.0  \n",
            "\n",
            "[5 rows x 49 columns]\n",
            "\n",
            "--- File: intensity_analysis_switched_head_tail.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  ini_frame  last_frame\n",
            "0         250        586         598\n",
            "1         256        458         492\n",
            "2         305        629         639\n",
            "3         360        570         578\n",
            "Error reading the file 'timestamp_raw.csv': [Errno 2] No such file or directory: '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_raw.csv'\n",
            "Error reading the file 'timestamp_time.csv': [Errno 2] No such file or directory: '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Datasets/timestamp_time.csv'\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Datasets extracted from the groups"
      ],
      "metadata": {
        "id": "UZGS8_6nUK_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_output_directory = '/content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Groups'\n",
        "\n",
        "print(f\"\\n{'='*30}\")\n",
        "print(f\"Visualizing the first 5 rows of the datasets in: {csv_output_directory}\")\n",
        "print(f\"{'='*30}\")\n",
        "try:\n",
        "    # Check if the output directory exists\n",
        "    if not os.path.exists(csv_output_directory):\n",
        "        print(f\"Error: The directory '{csv_output_directory}' was not found.\")\n",
        "        exit()\n",
        "\n",
        "    # Iterate through all files in the specified directory\n",
        "    for filename in os.listdir(csv_output_directory):\n",
        "        if filename.endswith(\".csv\"):  # Check if the file is a CSV file\n",
        "            file_path = os.path.join(csv_output_directory, filename)\n",
        "            try:\n",
        "                # Read the CSV file into a Pandas DataFrame\n",
        "                df = pd.read_csv(file_path)\n",
        "\n",
        "                print(f\"\\n--- File: {filename} ---\")\n",
        "                print(\"First 5 rows:\")\n",
        "                if not df.empty:\n",
        "                    print(df.head())\n",
        "                else:\n",
        "                    print(\"The DataFrame is empty.\")\n",
        "\n",
        "            except pd.errors.EmptyDataError:\n",
        "                print(f\"Warning: The file '{filename}' is empty.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading the file '{filename}': {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "print(\"\\nDataset visualization process completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9KGUsfdU0T3",
        "outputId": "b94f72b7-d1e1-4ec9-a7b0-513c74c0d25c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Visualizing the first 5 rows of the datasets in: /content/drive/MyDrive/Worms/Resultados/L1/L1_skeletons/Groups\n",
            "==============================\n",
            "\n",
            "--- File: intensity_analysis_switched_head_tail.csv ---\n",
            "First 5 rows:\n",
            "   worm_index  ini_frame  last_frame\n",
            "0         250        586         598\n",
            "1         256        458         492\n",
            "2         305        629         639\n",
            "3         360        570         578\n",
            "\n",
            "--- File: timestamp_raw.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: timestamp_time.csv ---\n",
            "First 5 rows:\n",
            "    0\n",
            "0 NaN\n",
            "1 NaN\n",
            "2 NaN\n",
            "3 NaN\n",
            "4 NaN\n",
            "\n",
            "--- File: provenance_tracking_scalars.csv ---\n",
            "First 5 rows:\n",
            "             name                                              value\n",
            "0      BLOB_FEATS  b'{\"func_name\": \"getBlobsFeats\", \"func_argumen...\n",
            "1  INT_SKE_ORIENT  b'{\"func_name\": \"correctHeadTailIntensity\", \"f...\n",
            "2      SKE_CREATE  b'{\"func_name\": \"trajectories2Skeletons\", \"fun...\n",
            "3        SKE_FILT  b'{\"func_name\": \"getFilteredSkels\", \"func_argu...\n",
            "4        SKE_INIT  b'{\"func_name\": \"processTrajectoryData\", \"func...\n",
            "\n",
            "--- File: provenance_tracking_scalars (1).csv ---\n",
            "First 5 rows:\n",
            "             name                                              value\n",
            "0      BLOB_FEATS  b'{\"func_name\": \"getBlobsFeats\", \"func_argumen...\n",
            "1  INT_SKE_ORIENT  b'{\"func_name\": \"correctHeadTailIntensity\", \"f...\n",
            "2      SKE_CREATE  b'{\"func_name\": \"trajectories2Skeletons\", \"fun...\n",
            "3        SKE_FILT  b'{\"func_name\": \"getFilteredSkels\", \"func_argu...\n",
            "4        SKE_INIT  b'{\"func_name\": \"processTrajectoryData\", \"func...\n",
            "\n",
            "Dataset visualization process completed.\n"
          ]
        }
      ]
    }
  ]
}